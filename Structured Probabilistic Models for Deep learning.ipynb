{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning involves learning probability distributions over large number of random variables. For example in a classification problem, the input is mapped into a probability distribution over all the classes. In some cases, using a single function to describe everything might not be fesiable at all. In such cases, we tend to use structured probabilistic models. What if we can efficiently capture the whole probability distribution using fewer parameters by assuming or inferring some properties of the underlying data?  This underlying probability distribution that we are trying to capture could be a lot of things, from images of galaxies to audio signals generated by Whales. Is there a general principle that could be used to capture the distribution over such a huge parameter space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Types of Graphical Models: Directed, Undirected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Directed graphical models are used when you have enough information about the underlying data distribution. If you sort of know what variables are effecting what other variables, you can build a directed graph where each node is a random variable and the edges represent conditional dependence on each other.\n",
    "\n",
    "$$ P(a,b,c,d,e) = P(a) P(b|a) P(c| a,b) P(d|b) P(e|c) $$\n",
    "\n",
    "* The conditional dependencies are acqurired from the data and a graph is built with it.\n",
    "\n",
    "* Undirected models does not assume an underlying conditional dependence among it's nodes, instead it tries to come up with functions that can approximate the probability distribution between the nodes. These functions need not be proabability distributions, but in the end you normalize the whole distribution so they have to be normalizable atleast.\n",
    "\n",
    "$$ P(X) = \\Pi F^i C^i $$\n",
    "\n",
    "* The whole probability is assumed to be coming from individual cliques in a graph, where each clique is parametrized by a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical Models Application Areas:\n",
    "\n",
    "* Density estimation (What is the true underlying distribution?)\n",
    "* Denoising data (Finding a mapping from noisy X to real X)\n",
    "* Missing value imputation (if you estimate the density of the true distribution, that could be used to fill up missing values)\n",
    "* Image Genenerative Models\n",
    "* Approximating large Graphs with fewer number of parameters\n",
    "* Generative Art!\n",
    "* Generative Music!\n",
    "* Brainstroming Models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges in using standard PGM's:\n",
    "\n",
    "* Trade-off between memomry and statistical effficiency: If we want to compute the values of probablity for all the values of random variables, it would consume too much memory. On the other hand, if you approximate too much, you lose efficiency. An example would be computing all the possible n-grams vs smoothing approaches. Coming up with a good trade-off is touch and go.\n",
    "\n",
    "* Runtime: Using full model might make it compute exhaustive, there are two kinds of runtime costs, one is during inference, and the other would be sampling. Standards PGM's are not so good at both of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Graphs to describe models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directed Models (also called Belief Networks, Bayesian Networks):\n",
    "\n",
    "* Alice (t0) -> Bob (t1) -> Carol (t2)\n",
    "\n",
    "* Dependencies are established with the graph.\n",
    "\n",
    "* The overall PD can be established as P(t0, t1, t2) = P(t0) P(t1|t0) P(t2| t1)\n",
    "\n",
    "* Parameter reduction (if each RV takes 100 values): Without graph = 100x100x100, with dependencies between edges = 99 + 9900 + 9900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undirected Models (also called Markov Random Fields, Markov Networks):\n",
    "\n",
    "* No CPD \n",
    "* Used when you don't have prior preference over choosing a casual relationship\n",
    "* The partition function or normalization constant (z) has close relations from statistical mechanics?\n",
    "* This 'Z' is often tough to compute practically. You should make sure that this function would give you a probabilities in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
