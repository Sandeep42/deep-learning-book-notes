{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning involves learning probability distributions over large number of random variables. For example in a classification problem, the input is mapped into a probability distribution over all the classes. In some cases, using a single function to describe everything might not be fesiable at all. In such cases, we tend to use structured probabilistic models. What if we can efficiently capture the whole probability distribution using fewer parameters by assuming or inferring some properties of the underlying data?  This underlying probability distribution that we are trying to capture could be a lot of things, from images of galaxies to audio signals generated by Whales. Is there a general principle that could be used to capture the distribution over such a huge parameter space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Types of Graphical Models: Directed, Undirected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Directed graphical models are used when you have enough information about the underlying data distribution. If you sort of know what variables are effecting what other variables, you can build a directed graph where each node is a random variable and the edges represent conditional dependence on each other.\n",
    "\n",
    "$$ P(a,b,c,d,e) = P(a) P(b|a) P(c| a,b) P(d|b) P(e|c) $$\n",
    "\n",
    "* The conditional dependencies are acqurired from the data and a graph is built with it.\n",
    "\n",
    "* Undirected models does not assume an underlying conditional dependence among it's nodes, instead it tries to come up with functions that can approximate the probability distribution between the nodes. These functions need not be proabability distributions, but in the end you normalize the whole distribution so they have to be normalizable atleast.\n",
    "\n",
    "$$ P(X) = \\Pi F^i C^i $$\n",
    "\n",
    "* The whole probability is assumed to be coming from individual cliques in a graph, where each clique is parametrized by a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical Models Application Areas:\n",
    "\n",
    "* Density estimation (What is the true underlying distribution?)\n",
    "* Denoising data (Finding a mapping from noisy X to real X)\n",
    "* Missing value imputation (if you estimate the density of the true distribution, that could be used to fill up missing values)\n",
    "* Image Genenerative Models\n",
    "* Approximating large Graphs with fewer number of parameters\n",
    "* Generative Art!\n",
    "* Generative Music!\n",
    "* Brainstroming Models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges in using standard PGM's:\n",
    "\n",
    "* Trade-off between memomry and statistical effficiency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
