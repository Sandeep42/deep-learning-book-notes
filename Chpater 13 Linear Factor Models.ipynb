{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed latent representations are what makes deep learning powerful. This paradigm of representation learning isn't new, almost every corner of machine learning is involved with learning some useful representation to do the task in hand. Linear Factor Models are the first kind of representation learning methods, they try to capture the underlying distribution of random varaible 'x' by using a latent variable 'h'.\n",
    "\n",
    "$$ p_{\\text{model}}(x) = \\mathbb{E}_{h} p_{\\text{model}}(x|h) $$\n",
    "\n",
    "As the name suggests, linear factor models model the world using a linear affine map.\n",
    "\n",
    "$$ x = Wx+b+ noise $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic PCA and Factor Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PCA and Factor models are linear representation models following the same logic as above, but only differ in the priors they set on 'h' and the noise term.\n",
    "* Factor analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
