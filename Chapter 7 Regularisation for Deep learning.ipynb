{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A learned model has to perform well not only on training data, but also on unseen data. To ensure that the model did not overfit and generalize well, regularization is a prominently used method. To put it bluntly, regularization is any additional parameters or noise added to the learning model so as to make it more robust and general. \n",
    "\n",
    "Typically, regularization is a penalty added to the parameter space. Adding an extra term to the loss function puts an additional constraint on the optimization objective. Regularization is generally not applied to the bias term.\n",
    "\n",
    "$$ \\widetilde J(\\theta; X,y) = J(\\theta; X,y) + \\alpha * \\omega(\\theta) $$\n",
    "\n",
    "In deep learning, is is quite common to use different $ \\alpha $ to different layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ L^2$ (Ridge) Regularization \n",
    "\n",
    "$$ \\widetilde J(\\theta; X,y) = J(\\theta; X,y) + 0.5 * \\alpha * \\omega^T \\omega $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
